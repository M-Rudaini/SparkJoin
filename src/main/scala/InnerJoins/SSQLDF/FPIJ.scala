package InnerJoins.SSQLDF
import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
//  Initializing the DF Full-Processed Inner-Join.
object FPIJ {
  //  Defining the main method.
  def main(args: Array[String]): Unit = {
    // Setting the logger Levels...
    Logger.getLogger("org").setLevel(Level.ERROR)
    Logger.getLogger("akka").setLevel(Level.ERROR)
    // Initializing Spark Session
    val ss = SparkSession
      .builder
      .appName("DF_FPIJ")
      .getOrCreate()
    // To use spark session implicit tools like the "$" operator...
    import ss.implicits._
    // loading Users DataFrame...
    // Considering the header as Column Names that will be used later...
    val InUserDF = ss.read.option("header","true")
     .csv(args(0)) // Source Connector.
    // Selecting the Required Columns from the Users DataFrame...
    // Concatenate first and last names as new column "Full_Name"...
    .select($"User_ID", concat($"First_Name", lit(" "), $"Last_Name")as "Full_Name")
      .persist()   //  Caching the DataFrame contents for further use...
    // Print the first 10 tuples...
    InUserDF.show(10)
    // loading Purchases DataFrame...
    // Considering the header as Column Names that will be used later...
    val InPurchseDF = ss.read.option("header","true")
    .csv(args(1))  // Source Connector.
    // Selecting the Required Columns from the Users DataFrame...
    //  to not conflict with the one in the Users DataFrame...
    .select($"User_ID", $"Unit_Amount" * $"Unit_Price" as "Total_Price")
    //  Grouping by  the User ID...
     .groupBy("User_ID")
    //  Summing all grouped total prices as total purchases ...
    .agg(sum("Total_Price") as "Total_Purchase")
    .persist()      //  Caching the DataFrame contents for further use...
    InPurchseDF.show(10)
    //  InPurchseDF is now a Dataset
    //  Joining the two Datasets...
    val UserPurchase = InUserDF.join(InPurchseDF, "User_ID")
    .persist()  //  Caching the DataFrame contents for further use...
    // Print the first 10 tuples...
    UserPurchase.show(10)
    // Saving the joined DataFrame to storage...
    UserPurchase.write.csv(args(2)+"/out.csv")
    // Stopping the Spark Session...
    ss.stop()
  }
}

